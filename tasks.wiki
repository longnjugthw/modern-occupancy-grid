== Tasks (priority sorted) ==
* [ ] Write paper
    * [ ] Address TODO's in paper ~/wrk/OccupancyGrid/doc/icra2014/modern_map.tex
* [.] Run algorithms on more datasets http://cres.usc.edu/radishrepository/view-all.php
    * [.] Run algorithms on all player datasets available.
        * [X] cave
        * [X] hospital
        * [X] hospital_section
        * [ ] simple_rooms
        * [ ] uoa_robotics_lab
        * [ ] autolab
        * [ ] freiburg
        * [ ] sal2
        * [ ] SFU
    * [ ] Average them out??
* [ ] Implement Gaussian measurement model for
    * [ ] metropolis hastings
    * [ ] belief propagation ??
    * [ ] dual decomposition ??
* [o] Quantitative comparison of algorithms
    * [X] Compute energy in algorithms
    * [ ] Probabilistic difference of marginals from ground truth.
* [X] Try algorithms on real laser data.
    * [X] Download and convert data set
    * [X] Run dataset on CCR
* [o] Write down the description of algorithms in the way I implemented them. (especially dual decomposition)
    * [X] Dual decomposition
    * [ ] Belief propagation
* [o] Consider ways of making belief propagation/dual decomposition faster
    * [X] Treat maxed out lasers correctly so that all the empty space is in agreement.
    * [ ] Implement other methods of dual decomposition like Block gradient descent.
    * [ ] Consider changing the size of laser factors to the disagreeing part only ??
* [ ] Tree Re-weighted Belief propagation ??
* [ ] Try algorithms on sonar data. Would require a difference sensor model.
* [ ] Think of someway to compute, step size in dual decomposition. Use min marginals.
* [ ] Learn the factors by using training data. It would be interesting to explore learning on sonar data, but not that much on laser data.
* [ ] implement neighbor factor. Occupied regions are contiguous.
 
== Observations ==
All methods that use factor graphs can be initialized with two-assumption method that is very-very fast.
A method that returns the optimal assignment as thought by each factor irrespective of what the cells already think about themselves. 

In case of belief propagation that means setting initial initializing x-f messages as equally likely (0.5) and the update sequences being factor to x edges first. But that would still be slower than two-assumption method because each f->x edge will just update one cell at a time not all the connected cells at a time.

In case of dual decomposition this might be true because we assign the cells at a bunch and then look for disagreements.

