== Tasks (priority sorted) ==
* [ ] Write down the description of algorithms in the way I implemented them. (especially dual decomposition)
* [ ] Implement Gaussian measurement model for
    * [ ] metropolis hastings
    * [ ] belief propagation ??
    * [ ] dual decomposition ??
* [o] Quantitative comparison of algorithms
    * [X] Compute energy in algorithms
    * [ ] Probabilistic difference of marginals from ground truth.
* [o] Consider ways of making belief propagation/dual decomposition faster
    * [X] Treat maxed out lasers correctly so that all the empty space is in agreement.
    * [ ] Implement other methods of dual decomposition like Block gradient descent.
    * [ ] Consider changing the size of laser factors to the disagreeing part only ??
* [ ] Tree Re-weighted Belief propagation ??
* [ ] Try algorithms on sonar data
* [ ] Learn the factors by using training data. It would be interesting to explore learning on sonar data, but not that much on laser data.
* [ ] implement neighbor factor. Occupied regions are contiguous.
 
== Observations ==
All methods that use factor graphs can be initialized with two-assumption method that is very-very fast.
A method that returns the optimal assignment as thought by each factor irrespective of what the cells already think about themselves. 

In case of belief propagation that means setting initial initializing x-f messages as equally likely (0.5) and the update sequences being factor to x edges first. But that would still be slower than two-assumption method because each f->x edge will just update one cell at a time not all the connected cells at a time.

In case of dual decomposition this might be true because we assign the cells at a bunch and then look for disagreements.

